---
title: "MachineLearningProject"
author: "Christopher Smith"
date: "July 26, 2015"
output: html_document
---

## Introduction

The purpose of this project is to predict the manner in which a participant lifted a barbell given some information collected by an activity tracking device. There are 5 possible ways in which the participant could have lifted the barbell. The way the participant lifter the barbell in each instance is defined by the 'classe' variable in the data set. 

For the purpose of brevity, whenever I take a summary of the data in this project I don't print out the results. Rest assured that the conclusions I draw following each call to 'summary()' are supported by the results of each summary.

My solution to the problem involves using randomforests and 10-fold cross validation to predict the response variable. I predict the error rate on new data will be 0% based on the results of the cross validation.


## Data cleaning and preparation

```{r}

# read in the data
train <- read.csv("~/Coursera/Machine Learning/training.csv", stringsAsFactors = F)

```

```{r, eval=FALSE}
# check for NA values and anything else weird
summary(train)
```

```{r}
# looks like some values are divided by zero and some are empty
# define vectors with the indices of these values so we can remove them.
div.zero <-  apply(train, 2, function(x) which(x == "#DIV/0!"))
empty <- apply(train, 2, function(x) which(x == ""))

# for each column, remove the aforementioned indices
cols <- 1:ncol(train)
for(i in cols){
  train[,i][div.zero[[i]]] <- NA
  train[,i][empty[[i]]] <- NA
}

```

```{r, eval = FALSE}
# look at the data again
summary(train)

```

```{r}

# find the percent of NA values in each column
num.NA.by.column <- apply(train, MARGIN = 2, function(x) length(which(is.na(x))))
percent.NA.by.column <- sapply(num.NA.by.column, function(x) x / nrow(train))

# remove all the columns that have more than 50% NA values
good.cols <- which(percent.NA.by.column < 0.5)
train <- train[,good.cols]

#find how many instances that leaves us with
length(which(complete.cases(train)))

#all remaining instances are complete
train <- train[which(complete.cases(train)), ]

class(train$classe)
train$classe <- as.factor(train$classe)

```

```{r, eval=FALSE}
summary(train)

```

```{r}
# based on the summary, remove the columns that are text because they have too many levels to be treated as factors in a random forest
bad.cols <- which(colnames(train) %in% c("user_name", "new_window", "cvtd_timestamp"))
train <- train[,-bad.cols]

```


## Cross validated randomforest training and testing

```{r}

#generate 10 folds for cross validation
library(randomForest)
library(caret)
cv.folds <- createFolds(y = 1:nrow(train))
resp.col <- which(colnames(train) == "classe")

for (i in 1:length(cv.folds)){
  this.cv.fold <- cv.folds[[i]]
  this.test <- train[this.cv.fold, ]
  this.train <- train[-this.cv.fold, ]
  
  this.randomforest <- randomForest(x = this.train[,-resp.col], y = this.train[,resp.col], 
                                    xtest = this.test[,-resp.col], ytest = this.test[,resp.col], 
                                    keep.forest = T)
  
  assign(x = paste("rf" , i , sep = ""), value = this.randomforest, envir = .GlobalEnv)
}

#build a list of all 9 randomforests
rf.list <- list(rf1, rf2, rf3, rf4, rf5, rf6, rf7, rf8, rf9)

# for each, find the test set error for each class
# Each row corresponds to one type of barbell lift, and each column corresponds to one cross-validation fold
sapply(rf.list, function(x) x$test$confusion[,6])

```

# Final Conclusion: 

### Since all 9 folds had 0% misclassification for all possible classes, the predicted misclassification rate for new data is also 0%.


#### End of file --------------------------------------------------------------
